{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib \n",
    "import urllib2  \n",
    "import lxml.html\n",
    "from bs4 import BeautifulSoup \n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"product-name\">\n",
      "<h3>Fancy Feast® Classic Adult Cat Food</h3>\n",
      "</div>\n",
      "\n",
      "Fancy Feast® Classic Adult Cat Food\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing that it works on one url:\n",
    "\n",
    "test_url = 'http://www.petsmart.com/cat/food-and-health/food/fancy-feast-classic-adult-cat-food-12865.html?cgid=200004'\n",
    "r = urllib.urlopen(test_url)\n",
    "soup_url = BeautifulSoup(r, 'lxml') #'html.parser'\n",
    "\n",
    "#print type(soup_url)\n",
    "name_box = soup_url.find('div', attrs={'class': 'product-name'}) #type: beautifulsoup.tag\n",
    "print name_box\n",
    "\n",
    "new_name = name_box.get_text()\n",
    "print new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now for a list of urls. To develop code, use list of only two urls\n",
    "test_product_list = ['http://www.petsmart.com/cat/food-and-health/food/fancy-feast-classic-adult-cat-food-12865.html?cgid=200004', 'http://www.petsmart.com/cat/food-and-health/food/purina-friskies-classic-pate-cat-food-1610.html?cgid=200004']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'\\nFancy Feast\\xae Classic Adult Cat Food\\n', u'\\nPurina\\xae Friskies\\xae Classic Pat\\xe9 Cat Food\\n']\n",
      "[u'1221059', u'1221026']\n",
      "[u'$0.59', u'$0.48']\n",
      "[u' Wet Food\\r\\n', u' Wet Food\\r\\n']\n",
      "[u' Adult 1-10 yrs\\r\\n', u' Adult 1-10 yrs\\r\\n']\n",
      "[u' General Health\\r\\n', u' General Health\\r\\n']\n",
      "[u' COD, Sole & Shrimp\\r\\n', u' Mixed Grill\\r\\n']\n",
      "[u' COD\\xa0\\r\\n', u' Liver\\xa0\\r\\n']\n",
      "[u' 3oz\\r\\n', u' 5.5oz\\r\\n']\n",
      "[u' Cod, Liver, Meat by-Products, Fish, Fish Broth, Sole, Shrimp, \\r\\nArtificial and Natural Flavors, Guar Gum, Added Color (Red 3 and Other Color), Calcium \\r\\nPhosphate, Potassium Chloride, Salt, Zinc Sulfate, Thiamine Mononitrate, Vitamin E \\r\\nSupplement, Ferrous Sulfate, Niacin, Manganese Sulfate, Calcium Pantothenate, Vitamin A \\r\\nSupplement, Copper Sulfate, Menadione Sodium Bisulfite Complex (Source of Vitamin K \\r\\nActivity), Pyridoxine Hydrochloride, Riboflavin Supplement, Vitamin B-12 Supplement, \\r\\nBiotin, Folic Acid, Vitamin D-3 Supplement, Potassium Iodide. \\r\\n', u'\\n']\n",
      "[<p><b>Guaranteed Analysis:</b>\\n<br/>Crude Protein (min.) 13%\\r\\n<br/>Crude Fat (min.) 2%\\r\\n<br/>Crude Fiber (max.) 1.5%\\r\\n<br/>Moisture (max.) 78%\\r\\n<br/>Ash (max.) 3.5%\\r\\n<br/>Taurine (min.) 0.05%\\r\\n\\t\\t\\t\\t\\t</p>, None]\n"
     ]
    }
   ],
   "source": [
    "def scrape_product_info(product_list):\n",
    "    \n",
    "    '''\n",
    "    This function isolates several attributes of a pet food product\n",
    "    into separate variables from the url passed to it\n",
    "       \n",
    "    Input:\n",
    "    A list of urls of pet food product pages generated from crawler.py\n",
    "       \n",
    "    Output:\n",
    "    name(str), productID(int), price(str), food_type(str), life_stage(str), \n",
    "    health_consideration(str), flavor(str),primary_ingredient(str), package_weight(str), \n",
    "    ingredient_list(list) and nutritional_analysis(list)\n",
    "    '''\n",
    "\n",
    "    name = []\n",
    "    product_id = []\n",
    "    price = []\n",
    "    food_type =[]\n",
    "    life_stage = []\n",
    "    health_consideration = []\n",
    "    flavor = []\n",
    "    primary_ingredient = []\n",
    "    package_weight = []\n",
    "    ingredient_list = []\n",
    "    nutritional_analysis = []\n",
    "\n",
    "    #open product pages from the product url list created from the  crawler function \n",
    "    for url in product_list:\n",
    "        \n",
    "        # Open each url, turn each page into a Beautiful Soup object\n",
    "        r = urllib.urlopen(url)\n",
    "        soup_page = BeautifulSoup(r, 'lxml') #'html.parser'\n",
    "\n",
    "        #Isolate each variable of interest and store it into a list:\n",
    "        \n",
    "        #Name\n",
    "        name_box = soup_page.find('div', attrs={'class': 'product-name'})\n",
    "        name_var = name_box.get_text()  \n",
    "        name.append(name_var)\n",
    "    \n",
    "        #Product ID\n",
    "        prodID_box = soup_page.find('span', attrs={'class': 'productID'}) \n",
    "        productID_var = prodID_box.get_text()\n",
    "        product_id.append(productID_var)\n",
    "    \n",
    "        #Price\n",
    "        price_box = soup_page.find('span', attrs={'class': 'price-regular'})\n",
    "        price_var = price_box.get_text()\n",
    "        price.append(price_var)\n",
    "    \n",
    "        #several product attributes are held in this class\n",
    "        type_box = soup_page.find('div', attrs={'class': 'tab-content'})\n",
    "\n",
    "        #Food Type\n",
    "        food_type_var = type_box.find_all('b')[0].next_sibling \n",
    "        food_type.append(food_type_var)\n",
    "    \n",
    "        #Life Stage\n",
    "        life_stage_var = type_box.find_all('b')[1].next_sibling\n",
    "        life_stage.append(life_stage_var)\n",
    "    \n",
    "        #Health Consideration\n",
    "        health_consideration_var = type_box.find_all('b')[2].next_sibling\n",
    "        health_consideration.append(health_consideration_var)\n",
    "\n",
    "        #Flavor    \n",
    "        flavor_var = type_box.find_all('b')[3].next_sibling\n",
    "        flavor.append(flavor_var)\n",
    "\n",
    "        #Primary Ingredient\n",
    "        primary_ingredient_var = type_box.find_all('b')[4].next_sibling\n",
    "        primary_ingredient.append(primary_ingredient_var)\n",
    "    \n",
    "        #Package Weight\n",
    "        package_weight_var = type_box.find_all('b')[5].next_sibling\n",
    "        package_weight.append(package_weight_var)\n",
    "    \n",
    "        #Ingredient List\n",
    "        ingredient_list_var = type_box.find_all('b')[7].next_sibling\n",
    "        ingredient_list.append(ingredient_list_var)\n",
    "    \n",
    "        #going 2 ps down the tree to pull out nutritional information\n",
    "        nutritional_analysis_var = type_box.findAll('p')[2].next_sibling\n",
    "        nutritional_analysis.append(nutritional_analysis_var)\n",
    "\n",
    "        return  name, product_id, price, food_type, life_stage, health_consideration, \\\n",
    "                flavor, primary_ingredient, package_weight, ingredient_list,          \\\n",
    "                nutritional_analysis\n",
    "        \n",
    "scrape_product_info(test_product_list)\n",
    "\n",
    "\n",
    "print name\n",
    "print product_id\n",
    "print price\n",
    "print food_type\n",
    "print life_stage\n",
    "print health_consideration\n",
    "print flavor\n",
    "print primary_ingredient\n",
    "print package_weight\n",
    "print ingredient_list\n",
    "print nutritional_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name Address",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-ab97a2ab7f8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msqlalchemy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msqlalchemy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msessionmaker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msqlalchemy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPerson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name Address"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "#from sqlalchemy import Address, Base, Person\n",
    "\n",
    "Base.metadata.bind = engine\n",
    " \n",
    "DBSession = sessionmaker(bind=engine)\n",
    "# A DBSession() instance establishes all conversations with the database\n",
    "# and represents a \"staging zone\" for all the objects loaded into the\n",
    "# database session object. Any change made against the objects in the\n",
    "# session won't be persisted into the database until you call\n",
    "# session.commit(). If you're not happy about the changes, you can\n",
    "# revert all of them back to the last commit by calling\n",
    "# session.rollback()\n",
    "session = DBSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, String, Integer, ForeignKey\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    " \n",
    "##add in the table set up from below\n",
    "    \n",
    "from sqlalchemy.orm import sessionmaker\n",
    " \n",
    "# Construct a sessionmaker object\n",
    "session = sessionmaker()\n",
    " \n",
    "# Bind the sessionmaker to engine\n",
    "session.configure(bind=engine)\n",
    " \n",
    "# Create all the tables in the database which are\n",
    "# defined by Base's subclasses such as User\n",
    "Base.metadata.create_all(engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'\\nFancy Feast\\xae Classic Adult Cat Food\\n', u'\\nPurina\\xae Friskies\\xae Classic Pat\\xe9 Cat Food\\n']\n",
      "[u'1221059', u'1221026']\n",
      "[u'$0.59', u'$0.48']\n",
      "[u' Wet Food\\r\\n', u' Wet Food\\r\\n']\n",
      "[u' Adult 1-10 yrs\\r\\n', u' Adult 1-10 yrs\\r\\n']\n",
      "[u' General Health\\r\\n', u' General Health\\r\\n']\n",
      "[u' COD, Sole & Shrimp\\r\\n', u' Mixed Grill\\r\\n']\n",
      "[u' COD\\xa0\\r\\n', u' Liver\\xa0\\r\\n']\n",
      "[u' 3oz\\r\\n', u' 5.5oz\\r\\n']\n",
      "[u' Cod, Liver, Meat by-Products, Fish, Fish Broth, Sole, Shrimp, \\r\\nArtificial and Natural Flavors, Guar Gum, Added Color (Red 3 and Other Color), Calcium \\r\\nPhosphate, Potassium Chloride, Salt, Zinc Sulfate, Thiamine Mononitrate, Vitamin E \\r\\nSupplement, Ferrous Sulfate, Niacin, Manganese Sulfate, Calcium Pantothenate, Vitamin A \\r\\nSupplement, Copper Sulfate, Menadione Sodium Bisulfite Complex (Source of Vitamin K \\r\\nActivity), Pyridoxine Hydrochloride, Riboflavin Supplement, Vitamin B-12 Supplement, \\r\\nBiotin, Folic Acid, Vitamin D-3 Supplement, Potassium Iodide. \\r\\n', u'\\n']\n",
      "[<p><b>Guaranteed Analysis:</b>\\n<br/>Crude Protein (min.) 13%\\r\\n<br/>Crude Fat (min.) 2%\\r\\n<br/>Crude Fiber (max.) 1.5%\\r\\n<br/>Moisture (max.) 78%\\r\\n<br/>Ash (max.) 3.5%\\r\\n<br/>Taurine (min.) 0.05%\\r\\n\\t\\t\\t\\t\\t</p>, None]\n"
     ]
    }
   ],
   "source": [
    "#Adding exceptions\n",
    "def scrape_product_info(product_list):\n",
    "    \n",
    "    '''\n",
    "    This function isolates several attributes of a pet food product\n",
    "    into separate variables from the url passed to it\n",
    "       \n",
    "    Input:\n",
    "    A list of urls of pet food product pages generated from crawler.py\n",
    "       \n",
    "    Output:\n",
    "    name(str), productID(int), price(str), food_type(str), life_stage(str), \n",
    "    health_consideration(str), flavor(str),primary_ingredient(str), package_weight(str), \n",
    "    ingredient_list(list) and nutritional_analysis(list)\n",
    "    '''\n",
    "\n",
    "    name = []\n",
    "    product_id = []\n",
    "    price = []\n",
    "    food_type =[]\n",
    "    life_stage = []\n",
    "    health_consideration = []\n",
    "    flavor = []\n",
    "    primary_ingredient = []\n",
    "    package_weight = []\n",
    "    ingredient_list = []\n",
    "    nutritional_analysis = []\n",
    "\n",
    "    #open product pages from the product url list created from the  crawler function \n",
    "    for url in product_list:       \n",
    "        try: \n",
    "        \n",
    "            # Open each url, turn each page into a Beautiful Soup object\n",
    "            r = urllib.urlopen(url)\n",
    "            soup_page = BeautifulSoup(r, 'lxml') #'html.parser'\n",
    "\n",
    "            #Isolate each variable of interest and store it into a list:\n",
    "        \n",
    "           #Name\n",
    "            name_box = soup_page.find('div', attrs={'class': 'product-name'})\n",
    "            name_var = name_box.get_text()  \n",
    "            name.append(name_var)\n",
    "    \n",
    "            #Product ID\n",
    "            prodID_box = soup_page.find('span', attrs={'class': 'productID'}) \n",
    "            productID_var = prodID_box.get_text()\n",
    "            product_id.append(productID_var)\n",
    "    \n",
    "            #Price\n",
    "            price_box = soup_page.find('span', attrs={'class': 'price-regular'})\n",
    "            price_var = price_box.get_text()\n",
    "            price.append(price_var)\n",
    "    \n",
    "            #several product attributes are held in this class\n",
    "            type_box = soup_page.find('div', attrs={'class': 'tab-content'})\n",
    "\n",
    "            #Food Type\n",
    "            food_type_var = type_box.find_all('b')[0].next_sibling \n",
    "            food_type.append(food_type_var)\n",
    "    \n",
    "            #Life Stage\n",
    "            life_stage_var = type_box.find_all('b')[1].next_sibling\n",
    "            life_stage.append(life_stage_var)\n",
    "    \n",
    "            #Health Consideration\n",
    "            health_consideration_var = type_box.find_all('b')[2].next_sibling\n",
    "            health_consideration.append(health_consideration_var)\n",
    "\n",
    "            #Flavor    \n",
    "            flavor_var = type_box.find_all('b')[3].next_sibling\n",
    "            flavor.append(flavor_var)\n",
    "  \n",
    "            #Primary Ingredient\n",
    "            primary_ingredient_var = type_box.find_all('b')[4].next_sibling\n",
    "            primary_ingredient.append(primary_ingredient_var)\n",
    "    \n",
    "            #Package Weight\n",
    "            package_weight_var = type_box.find_all('b')[5].next_sibling\n",
    "            package_weight.append(package_weight_var)\n",
    "    \n",
    "            #Ingredient List\n",
    "            ingredient_list_var = type_box.find_all('b')[7].next_sibling\n",
    "            ingredient_list.append(ingredient_list_var)\n",
    "    \n",
    "            #going 2 ps down the tree to pull out nutritional information\n",
    "            nutritional_analysis_var = type_box.findAll('p')[2].next_sibling\n",
    "            nutritional_analysis.append(nutritional_analysis_var)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return  name, product_id, price, food_type, life_stage, health_consideration, \\\n",
    "                flavor, primary_ingredient, package_weight, ingredient_list,          \\\n",
    "                nutritional_analysis\n",
    "        \n",
    "scrape_product_info(test_product_list)\n",
    "\n",
    "\n",
    "print name\n",
    "print product_id\n",
    "print price\n",
    "print food_type\n",
    "print life_stage\n",
    "print health_consideration\n",
    "print flavor\n",
    "print primary_ingredient\n",
    "print package_weight\n",
    "print ingredient_list\n",
    "print nutritional_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define a DB name\n",
    "dbname = 'pet_food'\n",
    "username = 'karigoodman'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres://karigoodman@localhost/pet_food\n"
     ]
    }
   ],
   "source": [
    "## 'engine' is a connection to a database\n",
    "## Here, we're using postgres, but sqlalchemy can connect to other things too.\n",
    "engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n",
    "print(engine.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "## create a database (if it doesn't exist)\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This section from http://newcoder.io/scrape/part-3/\n",
    "#define a table by inheriting from declarative base\n",
    "\n",
    "#models.py\n",
    "from sqlalchemy import create_engine, Column, Integer, String, DateTime\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.engine.url import URL\n",
    "\n",
    "\n",
    "DATABASE = {\n",
    "    'drivername': 'postgres',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432',\n",
    "    'username': 'karigoodman',\n",
    "    'password': '',\n",
    "    'database': 'pet_food'\n",
    "}\n",
    "\n",
    "#ITEM_PIPELINES = ['scraper_app.pipelines.PetSmartPipeline']\n",
    "\n",
    "DeclarativeBase = declarative_base()\n",
    "\n",
    "def db_connect():\n",
    "    \"\"\"\n",
    "    Performs database connection using database settings above.\n",
    "    Returns sqlalchemy engine instance\n",
    "    \"\"\"\n",
    "    return create_engine(URL(**settings.DATABASE))\n",
    "\n",
    "def create_pet_food_table(engine):\n",
    "    \"\"\"\n",
    "    Creates an empty table.\n",
    "    \"\"\"\n",
    "    DeclarativeBase.metadata.create_all(engine)\n",
    "\n",
    "class ProductAttributes(DeclarativeBase):\n",
    "    \"\"\"\n",
    "    Defines attributes of the table\n",
    "    \"\"\"\n",
    "    \n",
    "    __tablename__ = \"pet_food_table\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True)   \n",
    "    name = Column('title', String)\n",
    "    product_id = ('product_id', Integer)\n",
    "    price = ('price', String)\n",
    "    food_type = ('food_type')\n",
    "    life_stage = ('life_stage')\n",
    "    health_consideration = ('health_consideration')\n",
    "    flavor = ('flavor', String)\n",
    "    primary_ingredient = ('primary_ingredient', String)\n",
    "    ingredient_list = ('ingredient_list', String)\n",
    "    nutritional_analysis = ('nutritional_analysis', String)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pipeline.py\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from models import ProductAttributes, db_connect, create_pet_food_table #defined above\n",
    "\n",
    "\n",
    "class PetSmartPipeline(object):\n",
    "    \"\"\"PetSmart pipeline for storing scraped items in the database\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes database connection and sessionmaker.\n",
    "        Creates pet_food_table.\n",
    "        \"\"\"\n",
    "        engine = db_connect()\n",
    "        create_pet_food_table(engine)\n",
    "        self.Session = sessionmaker(bind=engine)\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        \"\"\"Save deals in the database.\n",
    "\n",
    "        This method is called for every item pipeline component.\n",
    "\n",
    "        \"\"\"\n",
    "        session = self.Session()\n",
    "        deal = Deals(**item)\n",
    "\n",
    "        try:\n",
    "            session.add(deal)\n",
    "            session.commit()\n",
    "        except:\n",
    "            session.rollback()\n",
    "            raise\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://pythoncentral.io/introductory-tutorial-python-sqlalchemy/\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    " \n",
    "from sqlalchemy_declarative import Address, Base, Person\n",
    " \n",
    "engine = create_engine('sqlite:///sqlalchemy_example.db')\n",
    "# Bind the engine to the metadata of the Base class so that the\n",
    "# declaratives can be accessed through a DBSession instance\n",
    "Base.metadata.bind = engine\n",
    " \n",
    "DBSession = sessionmaker(bind=engine)\n",
    "# A DBSession() instance establishes all conversations with the database\n",
    "# and represents a \"staging zone\" for all the objects loaded into the\n",
    "# database session object. Any change made against the objects in the\n",
    "# session won't be persisted into the database until you call\n",
    "# session.commit(). If you're not happy about the changes, you can\n",
    "# revert all of them back to the last commit by calling\n",
    "# session.rollback()\n",
    "session = DBSession()\n",
    " \n",
    "# Insert a Person in the person table\n",
    "new_person = Person(name='new person')\n",
    "session.add(new_person)\n",
    "session.commit()\n",
    " \n",
    "# Insert an Address in the address table\n",
    "new_address = Address(post_code='00000', person=new_person)\n",
    "session.add(new_address)\n",
    "session.commit()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
